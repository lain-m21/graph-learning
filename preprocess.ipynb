{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BlogCatalog data\n",
    "nodes_raw = pd.read_csv('./data/raw/blogcatalog/nodes.csv', header=None)\n",
    "edges_raw = pd.read_csv('./data/raw/blogcatalog/edges.csv', header=None)\n",
    "groups_raw = pd.read_csv('./data/raw/blogcatalog/groups.csv', header=None)\n",
    "groups_edges_raw = pd.read_csv('./data/raw/blogcatalog/group-edges.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = nodes_raw.values - 1\n",
    "edges = edges_raw.values - 1\n",
    "groups_edges_raw = groups_edges_raw - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_labels = pd.get_dummies(groups_edges_raw.sort_values(by=0), columns=[1]).groupby([0], as_index=True).sum().astype(int).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_dict = defaultdict(set)\n",
    "for e in edges:\n",
    "    adj_dict[e[0]].add(e[1])\n",
    "    adj_dict[e[1]].add(e[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_to_matrix(dic):\n",
    "    n = len(dic.keys())\n",
    "    adj_matrix = np.zeros([n, n], dtype=np.uint8)\n",
    "    for key in dic.keys():\n",
    "        nodes = np.array(list(dic[key]))\n",
    "        adj_matrix[key, nodes] = 1\n",
    "    return sparse.csr_matrix(adj_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_matrix = dict_to_matrix(adj_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse.save_npz('./data/preprocessed/blogcatalog/adj_matrix.npz', adj_matrix)\n",
    "with open('./data/preprocessed/blogcatalog/adj_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(adj_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/preprocessed/blogcatalog/node_labels.pkl', 'wb') as f:\n",
    "    pickle.dump(node_labels, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# facebook\n",
    "edges = []\n",
    "with open('./data/raw/facebook/facebook_combined.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "for l in lines:\n",
    "    edges.append([int(s) for s in l.strip().split()])\n",
    "edges = np.array(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_dict = defaultdict(set)\n",
    "for e in edges:\n",
    "    adj_dict[e[0]].add(e[1])\n",
    "    adj_dict[e[1]].add(e[0])\n",
    "    \n",
    "adj_matrix = dict_to_matrix(adj_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21.84550631344392"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(adj_matrix) / 2 / adj_matrix.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse.save_npz('./data/preprocessed/facebook/adj_matrix.npz', adj_matrix)\n",
    "with open('./data/preprocessed/facebook/adj_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(adj_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pubmed\n",
    "ind_allx = pickle.load(open('./data/raw/pubmed/ind.pubmed.allx', 'rb'), encoding='latin1')\n",
    "ind_ally = pickle.load(open('./data/raw/pubmed/ind.pubmed.ally', 'rb'), encoding='latin1')\n",
    "ind_tx = pickle.load(open('./data/raw/pubmed/ind.pubmed.tx', 'rb'), encoding='latin1')\n",
    "ind_ty = pickle.load(open('./data/raw/pubmed/ind.pubmed.ty', 'rb'), encoding='latin1')\n",
    "ind_x = pickle.load(open('./data/raw/pubmed/ind.pubmed.x', 'rb'), encoding='latin1')\n",
    "ind_y = pickle.load(open('./data/raw/pubmed/ind.pubmed.y', 'rb'), encoding='latin1')\n",
    "ind_graph = pickle.load(open('./data/raw/pubmed/ind.pubmed.graph', 'rb'), encoding='latin1')  # defaultdict(list) - adj_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/raw/pubmed/ind.pubmed.test.index', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "test_inds = []\n",
    "for l in lines:\n",
    "    test_inds.append(int(l))\n",
    "test_inds = np.array(test_inds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_dict = defaultdict(set)\n",
    "for key in ind_graph.keys():\n",
    "    adj_dict[key] = set(ind_graph[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_matrix = dict_to_matrix(adj_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse.save_npz('./data/preprocessed/pubmed/adj_matrix.npz', adj_matrix)\n",
    "with open('./data/preprocessed/pubmed/adj_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(adj_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inds = np.array([i for i in range(adj_matrix.shape[0]) if i not in test_inds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_features = np.zeros([adj_matrix.shape[0], ind_allx.shape[1]], dtype=np.float32)\n",
    "node_features[train_inds] = ind_allx.toarray()\n",
    "node_features[test_inds] = ind_tx.toarray()\n",
    "node_features = sparse.csr_matrix(node_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_labels = np.zeros([adj_matrix.shape[0], ind_ally.shape[1]], dtype=int)\n",
    "node_labels[train_inds] = ind_ally\n",
    "node_labels[test_inds] = ind_ty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse.save_npz('./data/preprocessed/pubmed/node_features.npz', node_features)\n",
    "with open('./data/preprocessed/pubmed/node_labels.pkl', 'wb') as f:\n",
    "    pickle.dump(node_labels, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ppi\n",
    "class_map = json.load(open('./data/raw/ppi/ppi-class_map.json', 'r'))\n",
    "id_map = json.load(open('./data/raw/ppi/ppi-id_map.json', 'r'))\n",
    "node_features = np.load('./data/raw/ppi/ppi-feats.npy')\n",
    "G = json.load(open('./data/raw/ppi/ppi-G.json', 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = nx.node_link_graph(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_dict = defaultdict(set)\n",
    "for edge in graph.edges:\n",
    "    adj_dict[edge[0]].add(edge[1])\n",
    "    adj_dict[edge[1]].add(edge[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adj_matrix = nx.adjacency_matrix(graph)\n",
    "adj_matrix = dict_to_matrix(adj_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = np.array(sorted([int(s) for s in id_map.keys()]))\n",
    "node_labels = np.zeros([len(nodes), len(class_map['0'])], dtype=int)\n",
    "for n in nodes:\n",
    "    node_labels[n, :] = np.array(class_map[str(n)]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_features = sparse.csr_matrix(node_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse.save_npz('./data/preprocessed/ppi/adj_matrix.npz', adj_matrix)\n",
    "with open('./data/preprocessed/ppi/adj_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(adj_dict, f)\n",
    "\n",
    "sparse.save_npz('./data/preprocessed/ppi/node_features.npz', node_features)\n",
    "with open('./data/preprocessed/ppi/node_labels.pkl', 'wb') as f:\n",
    "    pickle.dump(node_labels, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reddit\n",
    "class_map = json.load(open('./data/raw/reddit/reddit-class_map.json', 'r'))\n",
    "id_map = json.load(open('./data/raw/reddit/reddit-id_map.json', 'r'))\n",
    "node_features = np.load('./data/raw/reddit/reddit-feats.npy')\n",
    "G = json.load(open('./data/raw/reddit/reddit-G_full.json', 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = nx.node_link_graph(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_dict = defaultdict(set)\n",
    "for edge in graph.edges:\n",
    "    adj_dict[edge[0]].add(edge[1])\n",
    "    adj_dict[edge[1]].add(edge[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_matrix = dict_to_matrix(adj_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_ids = list(id_map.items())\n",
    "node_ids = sorted(node_ids, key=lambda x: x[1])\n",
    "class_size = max([class_map.items()]) + 1\n",
    "node_labels = np.zeros([len(node_ids), class_size], dtype=np.uint8)\n",
    "for n in node_ids:\n",
    "    node_labels[n[1], class_map[n[0]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_features = sparse.csr_matrix(node_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse.save_npz('./data/preprocessed/reddit/adj_matrix.npz', adj_matrix)\n",
    "with open('./data/preprocessed/reddit/adj_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(adj_dict, f)\n",
    "\n",
    "sparse.save_npz('./data/preprocessed/reddit/node_features.npz', node_features)\n",
    "with open('./data/preprocessed/reddit/node_labels.pkl', 'wb') as f:\n",
    "    pickle.dump(node_labels, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
